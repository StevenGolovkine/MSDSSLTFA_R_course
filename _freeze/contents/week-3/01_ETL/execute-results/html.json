{
  "hash": "19263ed2a7eef63b21e5875fc18b681f",
  "result": {
    "markdown": "---\ntitle: \"ETL Philosophy\"\nengine: knitr\n---\n\n\n<br>\nThe process of Extract, Transform, and Load (ETL) is a cornerstone of data warehousing. It commences with the extraction of data from heterogeneous sources, followed by the transformation of this data into a format that adheres to the prerequisites of the data warehouse. The final step involves the loading of the processed data into the designated repository.\n\n\n::: {#fig-etl}\n\n![](images/etl.svg)\n\nA schematic view of the ETL process.\n\n:::\n\nIn the following, we will explain each step of the process in more detail.\n\n## Extract Data\n\nWe can use diverse tools to extract data from different places. As we already saw, the `readr` package can be used to load regular tabular data, such as `.csv` file, and to work with Excel files, we will use the `readxl` package. However, real datasets are usually not in a flat file. Tabular data can be stored as an SQL database. In such a case, we may use the `DBI` or `odbc` package to establish the connection with the database and load the datasets. Nowadays, data providers commonly make their data accessible through APIs. You can use the `httr2` package to connect to an API. One may also want to do web scrapping. The commonly used package for doing so is the `rvest` package. Finally, if you are working with various cloud platforms, you may be interested with the `cloudyr` packages.\n\n## Transform Data\n\nData transformation involves performing a series of operations to prepare the data for analysis or integration into a database. These operations encompass a wide range of tasks, including but not limited to deduplicating records, managing missing values, rectifying data types, conducting aggregation, and engineering new features, etc. To accomplish these tasks, we use packages from the `tidyverse`.\n\n## Load Data\n\nIf the structure of the database is already in place, you can establish a connection with your designated data repository using the `DBI` or `odbc` packages. If the structure is not already in place, you usually have to create the necessary database tables or schemas to accommodate the transformed data. Once the foundations are in place, we can use the `DBI` package to efficiently load the transformed data into the target repository. To maintain data integrity throughout, implementing robust error handling and logging mechanisms is essential, helping you catch and address any potential issues during the loading process. For simple analysis, you may just need to export your data as an `.rds` file.\n\n## Others\n\nIn addition to the three fundamental steps of the ETL philosophy, there are a few additional parts to the overall process.\n\n### Automation and Scheduling\n\nAutomation is a key aspect of ETL processes, achieved by developing R scripts or functions that encapsulate the entire ETL pipeline. These automated scripts streamline the extraction, transformation, and loading steps, ensuring a transparent and efficient workflow. Furthermore, you can schedule these ETL jobs to run at fixed intervals, a essential aspect of maintaining data up to date. On Unix-like systems, the `cron` utility can be employed for this purpose, via the `cronR` package, while Windows users can utilise the `taskscheduleR` package (that use Windows Task Scheduler) to schedule and manage their ETL tasks.\n\n### Monitoring and Logging \n\nEffective ETL processes necessitate robust monitoring and logging systems. These systems are important for tracking the status and performance of ETL tasks, ensuring they run smoothly and efficiently. The `logger` package is used for logging or you can create custom logging functions tailored to your specific requirements. These logging mechanisms play a pivotal role in recording information, errors, and warnings, allowing for proactive intervention and the maintenance of data integrity throughout the ETL process.\n\n### Error Handling\n\nA resilient ETL system includes the integration of robust error-handling mechanisms to address any potential issues that may arise during the process. These mechanisms serve as a safety net, effectively managing errors and anomalies to ensure the continuity of data operations. In addition to handling errors, implementing well-considered retry strategies for transient errors is crucial. These strategies enable the ETL process to automatically reattempt certain operations in the event of non-persistent issues. Furthermore, it is essential to set up notifications for critical errors, facilitating immediate awareness and timely intervention, thus upholding data quality and process reliability.\n\nWe will delve into this topic in the chapter dedicated to [code debugging](/weeks/week-8.qmd).\n\n### Testing, Validation and Documentation\n\nRigorous testing and validation are essential to guarantee the accuracy and reliability of ETL transformations. This involves the development of comprehensive test cases and validation procedures to thoroughly assess the correctness of data operations. The `testthat` package is used to conduct unit tests and validation checks against predefined expected outcomes. Equally important is maintaining comprehensive documentation of the entire ETL methodology. This documentation encompasses essential details about data sources, transformation steps, and procedures employed for data loading. Additionally, documenting any business rules, assumptions, or dependencies embedded within the ETL process facilitates the transparency and ensures the sustainability of the methodology.\n\nWe will delve into this topic in the chapter dedicated to [reproducibility](/weeks/week-5.qmd).\n\n\n## Additional resources\n\n* The `DBI` [website](https://dbi.r-dbi.org).\n\n* The `odbc` [website](https://r-dbi.github.io/odbc/).\n\n* The `httr2` [website](https://httr2.r-lib.org/index.html).\n\n* The `rvest` [website](https://rvest.tidyverse.org).\n\n* The `cloudyr` project [website](https://cloudyr.github.io).\n\n* The `logger` [website](https://daroczig.github.io/logger/).\n\n* The `testthat` [website](https://testthat.r-lib.org).\n\n<br><br>\n\n::: {style=\"font-size: 0.875em;\"}\nBack to [week 03](/weeks/week-3.qmd) ‚èé\n:::",
    "supporting": [
      "01_ETL_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}