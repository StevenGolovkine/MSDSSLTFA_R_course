---
title: "Exploratory Data Analysis"
engine: knitr
---

```{r}
#| class-output: outputcode
#| eval: true
#| include: false
library(tidyverse)
```

<br>
Data is generated extensively across various domains. The challenge lies in extracting valuable insights and deriving meaningful conclusions from this data. The term **data analytics** is frequently employed interchangeably with **statistics**. Statistics is the science of learning from data that gives us the tools to answer interesting questions.

In the realm of data analytics, **domain knowledge** holds significant importance. It helps in the comprehension of variable meanings, the identification of important variables, and the recognition of those necessitating further cleaning. In cases where datasets originate from external teams, close collaboration with individuals possessing knowledge of where, how, who and why the data were collected, proves beneficial. Equally advantageous is the presence of a data codebook or documentation that explains the meanings and representations of different features within the dataset.

::: {.callout-note appearance="simple"}

The data collection process is the **most** step, yet it is sometimes overlooked or not thoroughly considered. Data collection frequently occurs without a systematic approach or proper consideration of the specific problem at hand. The quality of the data thus is essential, as without high-quality data, the effectiveness of data analytics, statistics, machine learning, or AI is severely compromised.

:::

## Statistics 

The initial stage of data analytics involves the generation of descriptive statistics, often referred to as exploratory analysis. In this step, we create simple summaries of our data to make it easier to understand, to describe any patterns in the data, or identify if any of our variables have relationships between them. It comprises two key components:

1. Data visualisation (which is the main focus of the chapter).

2. Numerical summaries (such as mean, median, standard deviation, variance, ...).


Finally, statistical inference is all about drawing generalisable conclusions from data when there is variability present. This process entails generalising the results to a wider group and relies on techniques such as hypothesis testing, confidence intervals, predictive modeling, and others to do so.

## Variability

We have a data set containing several variables. These variables contain information about the individuals we have measured them from. The values recorded for these variables will change from individual to individual. When we have different values, we have **variability**. When looking at data, that variability can be very large and it can obscure patterns (signals) that we might be interested in. Statistics helps us to distinguish the pattern (signal) from the variability (noise).

::: {.callout-note appearance="simple"}

## Example

Here is a simple example. The data come from an experiment carried out by Louis Pasteur who wanted to understand the impact of vaccines on anthrax, a disease that sheep suffered from. He took 48 sheep and split them into two equally sized groups. He gave one group the vaccine and the other nothing. He then infected all 48 sheep with anthrax and measured how many died in each group. The results are plotted in the bar plot.

```{r}
#| eval: true
#| echo: false
#| class-output: outputcode
df <- tribble(
    ~Vaccinated, ~Survive, ~Count,
    "Yes", "Yes", 24,
    "Yes", "No", 0,
    "No", "Yes", 0,
    "No", "No", 24
)

ggplot(df, aes(x = Vaccinated, y = Count)) +
    geom_col(aes(fill = Survive), position = 'dodge') + 
    scale_y_continuous(name = "Count") +
    theme_bw()
```

Here, the pattern is very clear because there is no variability. All of the sheep in the unvaccinated group died. None of the sheep in the vaccinated group died. 

:::

::: {.callout-note appearance="simple"}

## Example

The second example is more difficult because variability is present. Here, we have two types of the same component, A and B that are put into phones. The results are plotted in the bar plot.

```{r}
#| eval: true
#| echo: false
#| class-output: outputcode
df <- tribble(
    ~Component, ~Failure, ~Count,
    "A", "Failure", 12,
    "A", "No Failure", 6,
    "B", "Failure", 8,
    "B", "No Failure", 14,
)

ggplot(df, aes(x = Component, y = Count)) +
    geom_col(aes(fill = Failure), position = 'dodge') + 
    scale_y_continuous(name = 'Count') +
    theme_bw()

```

Now we can see that some phones from both types of the component failed but there were fewer failures for Type B. Is this difference a signal that Type B is better than Type A? Or is this just down to variability and if we took more phones with each type, we would see something else happening?

:::

To resume, in data analytics, we are interested in finding interesting and/or useful patterns in the data. Those patterns can be obscured by the variability in our data. But where does variability come from? 

There are two main sources of variation:

* Natural variation (which you can not control), it might be genetic, environmental or something else.

* Variation due to the measurement process: an instrument is less sensitive that we need, or something is copied and pasted incorrectly, etc.

It is important to keep these ideas in your mind when examining data.

::: {.callout-note appearance="simple"}

Please remember that this exploratory data analysis step is not used to generalise the results or trying to draw conclusions. It instead allows us to start understanding potential patterns in the data, identifying strange or outlying values or other sources of variability. We often use this step as a way of informing the models that we fit. 

:::

## Additional resources

* R4DS Exploratory Data Analysis [chapter](https://r4ds.hadley.nz/eda).


<br><br>

::: {style="font-size: 0.875em;"}
Back to [week 04](/weeks/week-4.qmd) ‚èé
:::