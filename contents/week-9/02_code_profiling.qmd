---
title: "Code Profiling in R"
engine: knitr
---

<br>
Code profiling is the process of analysing the performance of a program. It involves measuring the execution time of various parts of the code. Profiling helps to highlight which sections of the code consume the most resources and allows for more efficient coding.

Prior to optimising code for speed, it is crucial to ensure it is correct and readable. Instead of immediately pursuing faster performance, focus on creating code that functions correctly and is easy to understand.

## Profiling

We will use the `profvis` package to visualise aggregated profiling data, and it also links the profiling data back to the source code, facilitating a better understanding of necessary modifications. 
```{r}
#| eval: false
library(profvis)
```

After profiling, `profvis` generates an interactive HTML document for exploring the results. The upper section presents the source code along with bar graphs illustrating memory and execution time for each code line. This display offers a general sense of bottlenecks but may not always provide a precise cause. The lower section exhibits a flame graph that depicts the complete call stack. It allows you to observe the entire sequence of calls leading to each function. When using this display, you can hover over individual calls for additional details.

::: {.callout-note appearance="simple"}

## Example

```{r}
#| class-output: outputcode
#| eval: true
profvis::profvis({
  f <- function(x) {
      y <- integer()
      for (i in 1:1e4) {
        y <- c(y, i + x)
      }
      g(x)
      h(x)
  }

  g <- function(x) {
      y <- integer()
      for (i in 1:1e4) {
        y <- c(y, i + x)
      }
      h(x)
  }

  h <- function(x) {
      y <- integer()
      for (i in 1:1e4) {
        y <- c(y, i + x)
      }
      y
  }

  f(10)
})
```

:::

In a flame graph, there is a special entry, `<GC>`, which signifies the activity of the garbage collector. When `<GC>` consumes a substantial amount of time, it typically suggests the excessive creation of short-lived objects. To pinpoint the issue when the garbage collector is consuming a significant portion of the code's time, we can examine the memory column. Look for a segment where substantial memory is both allocated (on the right side of the bar) and subsequently freed (on the left side of the bar).

::: {.callout-note appearance="simple"}

## Example

```{r}
#| class-output: outputcode
#| eval: true
profvis::profvis({
  x <- integer()
  for (i in 1:1e4) {
    x <- c(x, i)
  }
})
```

:::

There are some limitations for the profiling: it cannot be extend to C/C++ code, anonymous are difficult to track as well as lazy evaluation.

## Microbenchmarking

A microbenchmark evaluates the speed of a tiny section of code, typically taking milliseconds (ms), microseconds (µs), or nanoseconds (ns) to execute. These benchmarks are handy for comparing small code snippets designed for specific tasks. However, it is important to be careful when applying the results of microbenchmarks to real-world code. Differences seen in microbenchmarks are often overshadowed by more significant factors in practical applications.

To benchmark the code, we will use the package `bench` that allows use to compare functions that only takes few nanoseconds.

```{r}
#| eval: false
library(bench)
```

Microbenchmark is quite easy to run using the `mark()` function. This function takes a list of functions as input, and perform the time comparison between each input function. It returns a tibble that we can then use for plotting or deriving statistics. The number of iterations is controlled with the `iterations` parameters.

::: {.callout-note appearance="simple"}

## Example

We compare the computation time for $x^4$ and $\exp(4\log(x))$ (which are equal).
It appears that $\exp(4\log(x))$ is faster to run.

```{r}
#| class-output: outputcode
#| eval: true
x <- runif(100)
bench::mark(exp(4 * log(x)), x^4, iterations = 1000)

```

:::

The default statistics that are computed are:

- `min`: the minimum execution time to run the expression.

- `median`: the median execution time to run the expression.

- `itr/sec`: an estimation of the number of executions per second.

- `mem_alloc`: the total amount of memory allocated by R while running the expression.

- `gc/sec`: the number of garbage collections per second.

The interpretation of the results is straighforward, we just have to pay attention to the units. If a microbenchmark takes:

- 1 ms, then one thousand calls take a second.
- 1 µs, then one million calls take a second.
- 1 ns, then one billion calls take a second.

In the example, both expressions take almost the time and changing one for another will not have a huge impact on the computation (except, maybe, if we need to run it bilions of times).

## Additional resources

* R package `proofvis` [website](https://rstudio.github.io/profvis/).

* R package `bench` [website](https://bench.r-lib.org).

<br><br>

::: {style="font-size: 0.875em;"}
Back to [week 09](/weeks/week-9.qmd) ⏎
:::