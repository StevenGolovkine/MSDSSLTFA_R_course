---
title: "Memory Issues"
editor: visual
---

Working with a big file, whose size is at least $50\%$ larger than avaialble RAM, but no more than tens or hundreds of gigabytes, may be difficult in R. By default, R loads files into memory (RAM) and so, if a file is bigger than the available RAM, it cannot load the file. Even if a file can fits into memory, when we start working on it, we may also start creating copies ot it (or create large matrices of output for some analysis) which may quickly overload the RAM.

Before starting reading the materials, download the data from [here](https://www.kaggle.com/datasets/wenxingdi/data-expo-2009-airline-on-time-data) and unzip it somewhere on your laptop. I would recommend to do the Part 1 of the tutorial as it might take (quite) a long time.

## Prepare

📖 [Numerical Oddities](/contents/week-7/01_numerical_oddities.qmd)

📖 [Memory](/contents/week-7/02_memory.qmd)

📖 [Large Files](/contents/week-7/03_large_files.qmd)

## Source code

🧑‍💻 [Source code](/contents/week-7/source/all_code_chunks.R)

## Questionnaire

⌨️ [Questionnaire](/questionnaire/qcm-week-7.qmd)

## Practice

📋 [Tutorials](/labs/lab-week-7.qmd)

⌨️ [Solution proposal](/labs/solutions/week-7.R)

<br><br>

::: {style="font-size: 0.875em;"}
Back to [course schedule](/ "Course schedule") ⏎
:::